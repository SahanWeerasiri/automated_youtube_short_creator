diff --git a/Telegram/bot2.py b/Telegram/bot2.py
new file mode 100644
index 0000000..ae9b4b0
--- /dev/null
+++ b/Telegram/bot2.py
@@ -0,0 +1,84 @@
+import os
+import dotenv
+import asyncio
+import json
+from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, WebAppInfo
+from telegram.ext import (
+    Application,
+    CommandHandler,
+    MessageHandler,
+    filters,
+    CallbackContext
+)
+
+# Load environment variables
+dotenv.load_dotenv()
+TOKEN = dotenv.get_key(dotenv.find_dotenv(), "BOT_TOKEN")
+
+async def start(update: Update, context: CallbackContext) -> None:
+    """Send welcome message with web app buttons."""
+    keyboard = [
+        [InlineKeyboardButton(
+            "AI Image Generator", 
+            web_app=WebAppInfo(url="https://perchance.org/unrestricted-ai-image-generator")
+        )],
+        [InlineKeyboardButton(
+            "Base64 Image Processor", 
+            web_app=WebAppInfo(url="https://v0-next-js-image-upload-gamma.vercel.app")
+        )]
+    ]
+    
+    await update.message.reply_text(
+        "Welcome! Choose a web app to use:",
+        reply_markup=InlineKeyboardMarkup(keyboard)
+    )
+
+async def handle_web_app_data(update: Update, context: CallbackContext) -> None:
+    """Handle data received from the web app."""
+    try:
+        web_app_data = update.effective_message.web_app_data
+        data = json.loads(web_app_data.data)
+        
+        # Process the received data
+        response_text = "âœ… Received data from web app:\n\n"
+        
+        # Format the data for display
+        if 'image' in data:
+            response_text += "ðŸ–¼ï¸ Image data received\n"
+            # You could save the image or process it further
+            await update.message.reply_photo(photo=data['image'])
+        elif 'text' in data:
+            response_text += f"ðŸ“ Text received: {data['text']}\n"
+        
+        # Add raw data for debugging
+        response_text += f"\nRaw data:\n{json.dumps(data, indent=2)}"
+        
+        await update.message.reply_text(response_text)
+        
+    except json.JSONDecodeError:
+        await update.message.reply_text("âš ï¸ Received invalid data from web app")
+    except Exception as e:
+        await update.message.reply_text(f"âŒ Error processing web app data: {str(e)}")
+
+async def main() -> None:
+    """Start the bot."""
+    # Create the Application
+    application = Application.builder().token(TOKEN).build()
+    
+    # Add handlers
+    application.add_handler(CommandHandler("start", start))
+    application.add_handler(MessageHandler(filters.StatusUpdate.WEB_APP_DATA, handle_web_app_data))
+    
+    # Run the bot
+    await application.initialize()
+    await application.start()
+    await application.updater.start_polling()
+    
+    print("Bot is running...")
+    
+    # Keep running until interrupted
+    while True:
+        await asyncio.sleep(3600)
+
+if __name__ == '__main__':
+    asyncio.run(main())
\ No newline at end of file
diff --git a/new/.env b/new/.env
new file mode 100644
index 0000000..642510b
--- /dev/null
+++ b/new/.env
@@ -0,0 +1,3 @@
+GEMINI_API_KEY=AIzaSyDJd4-d3kLsjkVrW78KYPgFeFR_YlMN2Xw
+CLAUDE_API_KEY=YOUR_CLAUDE_API_KEY
+GPT_API_KEY=YOUR_GPT_API_KEY
\ No newline at end of file
diff --git a/new/llm_agents/__pycache__/llm_agents.cpython-313.pyc b/new/llm_agents/__pycache__/llm_agents.cpython-313.pyc
new file mode 100644
index 0000000..49fba42
Binary files /dev/null and b/new/llm_agents/__pycache__/llm_agents.cpython-313.pyc differ
diff --git a/new/llm_agents/llm_agents.py b/new/llm_agents/llm_agents.py
new file mode 100644
index 0000000..47dda28
--- /dev/null
+++ b/new/llm_agents/llm_agents.py
@@ -0,0 +1,51 @@
+import requests
+import dotenv
+import os
+import sys
+sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+from models.llm_model import llm_request
+import logging
+
+dotenv.load_dotenv()
+
+GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
+CLAUDE_API_KEY = os.environ.get("CLAUDE_API_KEY")
+GPT_API_KEY = os.environ.get("GPT_API_KEY")
+
+def get_llm_response(llm_request: llm_request):
+    logging.basicConfig(level=logging.INFO)
+    logger = logging.getLogger(__name__)
+
+    logger.info(f"Preparing LLM request for model: {llm_request.model_name}")
+
+    if llm_request.model_name == "gemini":
+        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"
+    else:
+        url = f"https://generativelanguage.googleapis.com/v1beta/models/{llm_request.model_name}:generateContent?key={GEMINI_API_KEY}"
+    logger.debug(f"Request URL: {url}")
+
+    headers = {
+        "Content-Type": "application/json"
+    }
+    data = {
+        "contents": [
+            {
+                "parts": [
+                    {
+                        "text": llm_request.prompt
+                    }
+                ]
+            }
+        ],
+    }
+    logger.info(f"Sending request to LLM API with prompt: {llm_request.prompt}")
+
+    response = requests.post(url, headers=headers, json=data)
+    logger.info(f"Received response with status code: {response.status_code}")
+
+    if response.status_code != 200:
+        logger.error(f"Error: {response.status_code} - {response.text}")
+        raise ValueError(f"Error: {response.status_code} - {response.text}")
+
+    logger.debug(f"Response JSON: {response.json()}")
+    return response.json()
\ No newline at end of file
diff --git a/new/models/__pycache__/llm_model.cpython-313.pyc b/new/models/__pycache__/llm_model.cpython-313.pyc
new file mode 100644
index 0000000..ffefdcf
Binary files /dev/null and b/new/models/__pycache__/llm_model.cpython-313.pyc differ
diff --git a/new/models/__pycache__/prompt_model.cpython-313.pyc b/new/models/__pycache__/prompt_model.cpython-313.pyc
new file mode 100644
index 0000000..f6d5b66
Binary files /dev/null and b/new/models/__pycache__/prompt_model.cpython-313.pyc differ
diff --git a/new/models/llm_model.py b/new/models/llm_model.py
new file mode 100644
index 0000000..33944f9
--- /dev/null
+++ b/new/models/llm_model.py
@@ -0,0 +1,34 @@
+import json
+import os
+import sys
+sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+
+class llm_request:
+    def __init__(self, prompt: str, model_name: str, temperature: float = 0.7):
+        self.prompt = prompt
+        self.model_name = model_name
+        self.temperature = temperature
+        self.top_p = 0.9
+        self.top_k = 40
+        self.max_tokens = 1024
+        self.max_output_tokens = 1024
+        self.stop_sequences = []
+        self.return_likelihoods = "NONE"
+        self.response_format = "json"
+
+class llm_response_prompt_analysis:
+    def __init__(self, text: str):
+        print("Parsing LLM response for prompt analysis...")
+        text = text.strip()
+        if text.startswith("```json"):
+            text = text[7:].strip()
+        if text.endswith("```"):
+            text = text[:-3].strip()
+        text = json.loads(text)
+        self.app_name = text.get("app_name", "")
+        self.tech_stack = text.get("tech_stack", "")
+        self.features = text.get("features", [])
+        self.use_cases = text.get("use_cases", [])
+        self.target_audience = text.get("target_audience", "")
+        self.description = text.get("description", "")
+        self.other_details = text.get("other_details", "")
diff --git a/new/models/prompt_model.py b/new/models/prompt_model.py
new file mode 100644
index 0000000..9153f29
--- /dev/null
+++ b/new/models/prompt_model.py
@@ -0,0 +1,23 @@
+import os
+import sys
+sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+
+def prompt_analysis_prompt(prompt: str):
+    return f"""
+        You are an expert in analyzing software application prompts. Your task is to extract key information from the {prompt} prompt and return it in a structured JSON format.
+        Your response should include the following fields:
+        ```json
+        {{
+        "prompt_text": "<The original prompt text>",
+        "app_name": "<Name of the application>",
+        "tech_stack": "<Technology stack used>",
+        "features": ["<List of features>"],
+        "use_cases": ["<List of use cases>"],
+        "target_audience": "<Target audience>",
+        "description": "<Brief description of the application>",
+        "other_details": "<Any other relevant details>"
+        }}
+        ```
+        Make sure to provide a comprehensive analysis of the prompt, focusing on the application's purpose, its intended users, and any specific technologies or features mentioned.
+        Please ensure that the JSON is well-formed and includes all the required fields. If any field is not applicable or not mentioned in the prompt, you can leave it as an empty string or an empty list.
+        """
diff --git a/new/modules/prompt_analyzer.py b/new/modules/prompt_analyzer.py
new file mode 100644
index 0000000..b071bf6
--- /dev/null
+++ b/new/modules/prompt_analyzer.py
@@ -0,0 +1,47 @@
+import sys
+import os
+sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+from models.prompt_model import prompt_analysis_prompt
+from llm_agents.llm_agents import get_llm_response
+from models.llm_model import  llm_response_prompt_analysis, llm_request
+import logging
+
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+
+class prompt_analyzer:
+    def __init__(self, prompt: str):
+        self.prompt = prompt
+        logger.info("Initialized prompt_analyzer with prompt: %s", prompt[:100])
+
+    def analyze(self):
+        logger.info("Starting analysis for prompt.")
+        prompt = prompt_analysis_prompt(self.prompt)
+        logger.debug("Generated analysis prompt: %s", prompt)
+        request = llm_request(prompt, "gemini", temperature=0.7)
+        logger.debug("LLM request created: %s", request)
+        response = get_llm_response(request)
+        logger.info("Received response from LLM.")
+        if response and "candidates" in response and len(response["candidates"]) > 0:
+            text = response["candidates"][0]["content"]["parts"][0]["text"]
+            logger.debug("Extracted text from response: %s", text[:100])
+            result = llm_response_prompt_analysis(text)
+            logger.info("Analysis complete.")
+            return result
+        else:
+            logger.error("No valid response received from the LLM.")
+            raise ValueError("No valid response received from the LLM.")
+    
+    def get_prompt(self):
+        logger.info("Generating prompt analysis prompt.")
+        return prompt_analysis_prompt(self.prompt)
+
+if __name__ == "__main__":
+    # This block is for testing the module directly
+    # It can be removed or modified as needed for production use
+    test_prompt = """I need to build a web application for managing personal finances. The app should use React for the frontend and Node.js for the backend.
+    It should include features like budgeting, expense tracking, and financial reporting. need to save data in a real time database and should be
+    accessible on both web and mobile platforms. The target audience is young professionals who want to manage their finances effectively."""
+    analyzer = prompt_analyzer(test_prompt)
+    analysis_result = analyzer.analyze()
+    print(analysis_result)
\ No newline at end of file
